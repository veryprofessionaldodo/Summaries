VR Technology:

TECHNOLOGY MAKES THE BRAIN TICKLE HIHI

Stimulates as much senses to keep brain engaged:
- Visual (HMD or Projection for now, Cyberpunk 2077 implants in the future);

HMD are glorified displays with optics. They have:
- Lens (with FOV, Interpupillary distance, focal length, etc);
- Display;
- Ergonomics;

FOV is the angle that we can see, can be vertical, diagonal or horizontal.

HMDs can be monocular, bioccular (same thing for both eyes) or binocular.

Interpupillary distance (IPD) - distance between user eyes.

Eye Box can lead to reduced FOV.

Lens create distortions, but also maximize FOV.

Projection:
- Room Scale projection (CAVE);
- Dome Projection
- Vehicle simulator;

Stereo Projection:
- Active Stereo (shutter glasses, expensive);
- Passive Stereo (polarized images, cheaper);

Simulators combine VR with vehicle;

Haptic (for the feels yo):
- Touch Feedback: texture, temperature (etc);
- Force Feedback: weight and inertia;
- Passive Haptics use real props (cheap, large scale, accurate, but limited and not dynamic)
- Tactile feedback to stimulate skin tactile receptors;

Audio Display:
- Spatialization: make the sound from the right place;
- Localization: make the people say "uuuh, sound is coming from there";
- Properties:
    Number of channels;
    Masking;
    Localization;
    Sound stage;
- Head Related Transfer Functions model how the sound reaches the eardrums;

Tracking: 
- Registration: position virtual objects with real/virtual objects. Maybe an example is to hold an in-game object?
- Tracking: continuosly locate the users view/viewpoint;
- Mechanical trackers have high accuracy, haptic feedback, but are cumbersome and expensive;
- Magnetic Tracker use difference between magnetic transmitter and receiver;
- Inertial Tracker use accelerometer and gyroscope;
- Optical Tracker uses image processing and computer vision;
- Inside-out tracking uses the HMD to do the tracking (WMR);
- Outside-in tracking uses exterior sensors to know the positions of things (Vive);

Input devices bring interactivity to the virtual world.

Computer output (images, sounds, haptic feedback) is body input, body output (motion, voice) is the computer input.

Input Device characteristics:
- Size and shape, is it bad to hold;
- Degrees of freedom (axis it supports);
- Direct manipulation (like pinching to zoom on a phone, or dragging a folder to another place, requires direct manipulation and continuous update of the target) vs indirect manipulation (like pop up menus that appear without direct user control);
- Relative input (difference between previous and current input) vs absolute input (difference to static point of reference);
- Isometric (measure pressure or force with no actual movement) vs isotonic (measure difference from static point of reference);

Hand-input devices:
- World-grounded (joysticks, good for vehicle metaphor);
- Non-tracked (remote controllers);
- Tracked (Vive controllers, they are good for VR);
- Hand-worn (gloves, good for gestures);
- Hands :) (computer vision, required line of sight)

Non-hand input device:
- Eye tracking (shine IR light into eyes);
- Full-body VR (wearables);
- Walking 

VR System, user gives input, VR engine provides output.

Architecture:
- Application layer is the game logic and whatnot;
- Graphics layer is the physics, scene graph (nodes in the scene);
- Rendering layer is the low level graphics code (OpenGL, DirectX, Vulkan), responsible with determining what color the pixels will be;

There is a delay between the action and the output of said action.

Can cause sickness, remove immersion, and add motion blur.