Integration testing:
- Testing of the integrations of components that create a sub-system;
- Difficult to localise errors;
- Should be done by an independent testing team;

Interface types: 
- Parameter interfaces (data passed from one to another);
- Shared memory interfaces (shared block of memory);
- Procedural interfaces (sub-system encapsulates set of procedures that another sub-systems calls);
- Message passing interfaces (sub-systems request services from other sub-systems);

Errors:
- Misuse;
- Misunderstanding (incorrectly assumes behaviour);
- Timing;

Guidelines:
- Use extreme range of values in parameter testing;
- Test null pointers;
- Design test to make the component fail;
- Stress test message passing systems;
- Vary the order in which components are activated in shared memory systems;

Big-bang testing:
- All components are connected, harder to isolate errors;
- Alternative is to do incremental integration testing;

Incremental Integration testing:
- Top-down (start from the higher level, use stubs for lower levels);
- Bottom-up (integrate bottom components until everything is connected);
- Sandwich (both of the above);
- Collaboration (scenario based testing);
- Thread testing (integration testing follows requirements, not hierarchy);
- Functional Integration (combines components to get basic functionality early);
- Neighborhood (only the nodes that connect a certain node are tested);
- Pairwise (similar to above, but in pairs);

It must follow software construction strategy.

Test harness: auxiliary code to help testing;
Test drivers: calls the code;
Test stubs: simulate the real thing, can use mock objects;

Top-down vs bottom-up:
- Top-down can identify design flaws earlier;
- Bottom-up can provide more demonstration earlier;
- Top-down needs complex stubs, bottom-up needs drivers. Sandwich combines both;
- Problems with observation of tests for both cases;
- Top-down can test data flow and logic for that early, bottom-up can't;

Mixed Approach:
- THE BEEEEST OF BOTH WORLDS;
- Allows early release of limited functionality;
- Minimizes need for stubs and drivers;

Problems: 
- Not enough environments (can be muy expensivo);
- Not enough time;
- Test beds aren't accurate;
- Unavailable components;
- Poor code quality;

Unit testing vs integration testing:
- Fast vs maybe slow;
- Isolated vs more than one component;
- Can use mock vs only unrelated components can be mocked;
- failure means bad code vs failure may mean environment change;

System Testing:
- Testing the whole thing by an independent team;
- Mui expensivo;
- Based on requirements, both functional and non-functional;
- Goal is to test whether it does what it's supposed to, and well enough;
- Can use previous tests, but it may also need new tests to test the whole thing. Have I mentioned tests?;

Functional Testing:
- The behaviour adheres to requirements;
- Black-box;
- Traceability matrix;

Performance Testing:
- Does it meet the performance requirements;
- Results need quantification;

Stress Testing:
- Me rn, amirite fellow gamers;
- Maximize load in system just for the giggles;
- Minimize resources as well;
- Attemp to run the program in the worst possible scenarios, just to see if it can take it;

Configuration Testing: 
- Combinates hw and sw to see if it might break;
- Introduce errors in the devices just to see if the system can handle it;

Security Testing:
- Evaluates system characteristics that relate to the availability, integrity and confidentiality of system data and services;
- Responsibility of security specialist;

Recovery Testing:
- The most cruel of them all, remove resources to see if system can recover. Poor software;

Reliability and availability testing:
- Reliability is if the software will operate without failure for a given time and conditions;
- Availability is the probability the software is available for use;

Usability Testing: 
- opah, já vi isto no 3, já chega

Automated GUI testing approaches:
- Capture-replay:
    Captures input by an user for example, then replays it;
    Verifies the output by comparing bitmaps for example;
    Problem is, it can only be used after tool is working;
- Random input testing tools:
    Actions performed randomly;
    Dumb Monkeys keep trying things without recognizing an error, a smart monkey knows somewhat what was supposed to happen after input;
    Good to find crashes, but not much else;
- Unit testing frameworks:
    JA FALEI SOBRE ISTO 
- Model-based testing tools;
    Higher degree of automation, but with large amounts of tests since these are generated;
    
Regression Testing:
- Adds something, test to make sure nothing that worked before is broken;
- Can be automated, and is good to improve quality of product;
- If not automated, time consuming;

Regression makes sure that additions did not break previous product, retesting is just to make sure bug has been fixed. Only runs failed tests. 

